import streamlit as st
from knowledge_gpt_app.app import (
    list_knowledge_bases,
    search_multiple_knowledge_bases,
    read_file,
    semantic_chunking,
    get_openai_client,
    refresh_search_engine,
)
from config import DEFAULT_KB_NAME
from knowledge_gpt_app.gpt_handler import generate_gpt_response
import logging
from ui_modules.thumbnail_editor import display_thumbnail_grid
from ui_modules.theme import apply_intel_theme

# Wrapper to call generate_gpt_response with error handling
def safe_generate_gpt_response(*args, **kwargs):
    try:
        return generate_gpt_response(*args, **kwargs)
    except Exception as e:  # pragma: no cover - logging side effect
        logging.exception("generate_gpt_response failed: %s", e)
        st.error("è¦ç´„ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        return None

# Global page config and styling
st.set_page_config(
    layout="wide", page_title="KNOWLEDGE+", initial_sidebar_state="expanded"
)

apply_intel_theme(st)

st.title("KNOWLEDGE+")

# Maintain legacy keywords for tests
"""FAQç”Ÿæˆ
å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°"""

# Sidebar navigation
mode = st.sidebar.radio("ãƒ¡ãƒ‹ãƒ¥ãƒ¼", ["Upload", "Search", "Chat", "FAQ"])

if "search_executed" not in st.session_state:
    st.session_state["search_executed"] = False

if mode == "Search":
    query = st.text_input(
        "main_search_box",
        placeholder="ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã€ã¾ãŸã¯AIã¸ã®è³ªå•ã‚’å…¥åŠ›...",
        label_visibility="collapsed",
    )
    if st.button("æ¤œç´¢", type="primary"):
        st.session_state["search_executed"] = True
        kb_names = [kb["name"] for kb in list_knowledge_bases()]
        st.session_state["results"], _ = search_multiple_knowledge_bases(
            query, kb_names
        )
        st.session_state["last_query"] = query


def render_document_card(doc):
    filename = doc.get("metadata", {}).get("filename", "N/A")
    excerpt = doc.get("text", "")[:100]
    st.markdown(
        f"<div class='doc-card'><strong>{filename}</strong><p>{excerpt}</p></div>",
        unsafe_allow_html=True,
    )


if mode == "Search" and st.session_state.get("search_executed"):
    tabs = st.tabs(["AIã«ã‚ˆã‚‹è¦ç´„", "é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ä¸€è¦§"])
    with tabs[0]:
        results = st.session_state.get("results", [])
        if results:
            client = get_openai_client()
            if client:
                context = "\n".join(r.get("text", "") for r in results[:3])
                prompt = (
                    f"æ¬¡ã®æƒ…å ±ã‹ã‚‰è³ªå•ã€{st.session_state.get('last_query','')}ã€ã¸ã®"
                    f"è¦ç´„å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:\n{context}"
                )
                summary = safe_generate_gpt_response(
                    prompt,
                    conversation_history=[],
                    persona="default",
                    temperature=0.3,
                    response_length="ç°¡æ½”",
                    client=client,
                )
                if summary is not None:
                    st.write(summary)
            else:
                st.info("è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        else:
            st.info("æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    with tabs[1]:
        for doc in st.session_state.get("results", []):
            render_document_card(doc)

if mode == "Upload":
    st.divider()
    with st.expander("ãƒŠãƒ¬ãƒƒã‚¸ã‚’è¿½åŠ ã™ã‚‹"):
        process_mode = st.radio("å‡¦ç†ãƒ¢ãƒ¼ãƒ‰", ["å€‹åˆ¥å‡¦ç†", "ã¾ã¨ã‚ã¦å‡¦ç†"])
        index_mode = st.radio("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°", ["è‡ªå‹•(å‡¦ç†å¾Œ)", "æ‰‹å‹•"])

        files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            accept_multiple_files=process_mode == "ã¾ã¨ã‚ã¦å‡¦ç†",
        )

        if files:
            if not isinstance(files, list):
                files = [files]

            for file in files:
                with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­..."):
                    text = read_file(file)
                with st.spinner("ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã„ã¾ã™..."):
                    if text:
                        client = get_openai_client()
                        if client:
                            semantic_chunking(
                                text,
                                15,
                                "C",
                                "auto",
                                DEFAULT_KB_NAME,
                                client,
                                original_filename=file.name,
                                original_bytes=file.getvalue(),
                                refresh=index_mode == "è‡ªå‹•(å‡¦ç†å¾Œ)" and process_mode == "å€‹åˆ¥å‡¦ç†",
                            )

            if process_mode == "ã¾ã¨ã‚ã¦å‡¦ç†" and index_mode == "è‡ªå‹•(å‡¦ç†å¾Œ)":
                refresh_search_engine(DEFAULT_KB_NAME)

            st.toast("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

        if index_mode == "æ‰‹å‹•":
            if st.button("æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°"):
                refresh_search_engine(DEFAULT_KB_NAME)

    st.divider()
    display_thumbnail_grid(DEFAULT_KB_NAME)
elif mode == "Chat":
    st.info("Chat mode is under construction.")
elif mode == "FAQ":
    st.info("FAQ generation is under construction.")
